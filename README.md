# RL-Based-Document-Summarization
# A Reinforcement Learning Approach to Document Summarization

This project investigates the use of Reinforcement Learning (RL) for abstractive text summarization. The goal is to generate concise, coherent, and novel sentences that capture the essence of the original text, rather than simply extracting existing sentences. 

---

## Table of Contents
1.  [Project Overview](#project-overview)
2.  [Dataset](#dataset)
3.  [Methodology](#methodology)
4.  [Models](#models)
5.  [Evaluation Metric](#evaluation-metric)
6.  [Results](#results)
7.  [Challenges](#challenges)
8.  [How to Run](#how-to-run)

---

## Project Overview

Traditional text summarization can be broadly categorized into extractive and abstractive methods. Our focus is on abstractive summarization, which aims to produce more human-like summaries by understanding the content and generating new phrases. Reinforcement learning is employed to optimize the summarization models by treating the generation process as a sequence of decisions, where the model is rewarded based on the quality of the final summary.

This project implements two RL algorithms to tackle this task:
* **REINFORCE** 
* **Actor-Critic** 

---

## Dataset

The project utilizes the **Amazon Fine Food Reviews** dataset. This is a large corpus of user-generated reviews, ideal for training abstractive summarization models 43].

**Dataset statistics:**
* Total Reviews: 568,454 
* Total Users: 256,059 
* Total Products: 74,258 

---

## Methodology

The project follows a structured pipeline from data acquisition to model evaluation:

1.  **Data Acquisition:** The Amazon Fine Food Reviews dataset was acquired.
2.  **EDA and Data Cleanup:** Initial exploratory data analysis and cleaning were performed.
3.  **Text Preprocessing:** The text was cleaned through a series of steps
    * Removal of Hyperlinks  
    * Removal of Unicode text  
    * Removal of Emoji  
    * Removal of Punctuations 
    * Lowercasing 
    * Word Tokenization  
4.  **Data Preparation:** The preprocessed text was converted into a machine-readable format using Text to Sequence and Pad Sequence techniques.
5.  **Data Splitting:** The dataset was split into 70% for Training, 20% for Validation, and 10% for Testing.
6.  **Model Training:** The REINFORCE and Actor-Critic models were trained on the prepared data.
7.  **Evaluation:** The performance of the models was evaluated using ROUGE scores and human evaluation.

---

## Models

Two reinforcement learning architectures were implemented and trained.

### 1. REINFORCE Model

The REINFORCE model is a policy gradient method that directly optimizes the policy (the summary generator).
* **Agent:** A model responsible for generating and evaluating summaries.
* **Reward:** Calculated using the ROUGE score to provide feedback on summary quality. Reward clipping and normalization were used for stable training.
* **Loss Function:** Computed as the negative log probability of the selected actions (words), weighted by the rewards, to maximize the expected cumulative reward.
* **Optimizer:** The Adam optimizer was used to update model parameters.

### 2. Actor-Critic Model

This architecture uses two networks to improve learning stability.
* **Actor (Generator):** A neural network that generates the summaries. Its loss function measures how well it selects words based on rewards.
* **Critic (Evaluator):** A network that evaluates the summaries generated by the Actor Its loss function evaluates the accuracy of its own predictions about the quality of the summaries.
* **Reward & Optimizer:** Similar to the REINFORCE model, rewards were based on ROUGE scores, and the Adam optimizer was used for training.

---

## Evaluation Metric

### ROUGE-1 F1 Score

The primary metric used for evaluation is the **ROUGE-1 F1 score**.
* **Definition:** It measures the quality of a summary by comparing the overlap of unigrams (single words) between the machine-generated summary and a human-created reference summary.
* **Components:** It is the harmonic mean of Precision and Recall.
    * **Precision:** The ratio of the number of overlapping words to the total number of words in the generated summary.
    * **Recall:** The ratio of the number of overlapping words to the total number of words in the reference summary.

ROUGE is a standard for benchmarking summarization systems as it provides an automated, quantitative measure of summary quality.

---

## Results

Both models produced mixed results.
* **Good Results:** In several cases, the models generated single-word summaries like "beautiful" or "best" that were contextually related to the original summaries ("Great!", "Food-Great").
* **Bad Results:** In other instances, there was no clear relation between the generated summary and the original summary. For example, a review about chips received the generated summary "stranded" while the original was "great tasting cocoa powder".

---

## Challenges

Several challenges were identified during this project:

* **Sparse Rewards:** Metrics like ROUGE can provide delayed and infrequent feedback, which makes the learning process difficult.
* **Reward Design:** Creating a reward function that accurately captures all nuances of a high-quality summary is a significant challenge.
* **Exploration vs. Exploitation:** Finding the right balance between trying new summary strategies and sticking with known effective ones is complex.
* **Sample Efficiency:** RL algorithms often require a large amount of data to learn effectively, which is computationally expensive.
* **Model Stability:** Training was sometimes unstable due to issues like reward sparsity and vanishing gradients.

---

## How to Run

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/your-username/your-repo-name.git](https://github.com/your-username/your-repo-name.git)
    cd your-repo-name
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Run the training script:**
    ```bash
    python train.py --model [reinforce|actor_critic]
    ```

4.  **Run inference:**
    ```bash
    python summarize.py --text "Your text to summarize here."
    ```
